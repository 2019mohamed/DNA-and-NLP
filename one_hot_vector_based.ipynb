{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "one hot vector based.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPpxhcxOU/RQUnrd6bUmWH/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2019mohamed/DNA-and-NLP/blob/main/one_hot_vector_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9ccjS4kTpOu",
        "outputId": "a598ef22-6678-4179-ee8d-0de5c89cc117"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler , BatchSampler\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"MLP with linear output\"\"\"\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"MLP layers construction\n",
        "        Paramters\n",
        "        ---------\n",
        "        num_layers: int\n",
        "            The number of linear layers\n",
        "        input_dim: int\n",
        "            The dimensionality of input features\n",
        "        hidden_dim: int\n",
        "            The dimensionality of hidden units at ALL layers\n",
        "        output_dim: int\n",
        "            The number of classes for prediction\n",
        "        \"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "        self.linear_or_not = True  # default is linear model\n",
        "        self.num_layers = num_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if num_layers < 1:\n",
        "            raise ValueError(\"number of layers should be positive!\")\n",
        "        elif num_layers == 1:\n",
        "            # Linear model\n",
        "            self.linear = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            # Multi-layer model\n",
        "            self.linear_or_not = False\n",
        "            self.linears = torch.nn.ModuleList()\n",
        "            self.batch_norms = torch.nn.ModuleList()\n",
        "\n",
        "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
        "            for layer in range(num_layers - 2):\n",
        "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "            for layer in range(num_layers - 1):\n",
        "                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.linear_or_not:\n",
        "            # If linear model\n",
        "            return self.linear(x)\n",
        "        else:\n",
        "            # If MLP\n",
        "            h = x\n",
        "            for i in range(self.num_layers - 1):\n",
        "                h = F.relu(self.linears[i](h))\n",
        "            return self.linears[-1](h)\n",
        "\n",
        "class SeqData (Dataset):\n",
        "    def __init__(self):\n",
        "        data = pd.read_csv('promoters.csv')\n",
        "        data['Sequence'] = data['Sequence'].str.replace('\\t\\t' , '')\n",
        "        data['Sequence'] = data['Sequence'].str.replace('\\t' , '')\n",
        "        self.seqs = list(data['Sequence'])\n",
        "        #print(self.seqs[0])\n",
        "        self.maxlen = len(max(self.seqs , key = lambda k :len(k)))\n",
        "        self.labels = list(data['Class'])\n",
        "        self.map = {'a':0 , 'c':1 , 'g':2 , 't':3 }\n",
        "        #print(self.seqs[34],' ',self.labels[34])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = np.zeros((self.maxlen,len(self.map)))\n",
        "        seq = self.seqs[index].lower()\n",
        "        for i, alpa in enumerate(seq):\n",
        "            x[i,self.map[alpa]] = 1\n",
        "        #print(x)\n",
        "        l = 0 if self.labels[index] == '+' else 1\n",
        "        return x , l\n",
        "    \n",
        "    \n",
        "def train(net, trainloader, optimizer, criterion):\n",
        "    net.train()\n",
        "\n",
        "    running_loss = 0\n",
        "    total_iters = len(trainloader)\n",
        "\n",
        "    for idx , data  in enumerate(trainloader):\n",
        "\n",
        "        x, labels = data\n",
        "        outputs = net(x.float())\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    running_loss = running_loss / total_iters\n",
        "\n",
        "    return running_loss\n",
        "\n",
        "\n",
        "def eval_net(net, dataloader, criterion):\n",
        "    net.eval()\n",
        "\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for idx ,  data in enumerate(dataloader):\n",
        "        x, labels = data\n",
        "\n",
        "        total += len(labels)\n",
        "        outputs = net(x.float())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total_correct += (predicted == labels.data).sum().item()\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item() * len(labels)\n",
        "\n",
        "    loss, acc = 1.0*total_loss / total, 1.0*total_correct / total\n",
        "\n",
        "\n",
        "    return loss, acc\n",
        " \n",
        "    \n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        #self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first = True ) \n",
        "        self.rnn = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        c = self.init_hidden(batch_size)\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        #out, hidden = self.rnn(x, hidden)\n",
        "        out , hidden = self.rnn (x , (hidden , c))\n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out[: , -1 , :]\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
        "        return hidden       \n",
        "\n",
        "def split_rand(dataset,batch_size, split_ratio=0.7, seed=42, shuffle=True):\n",
        "    import math\n",
        "    num_entries = len(dataset)\n",
        "    indices = list(range(num_entries))\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(math.floor(split_ratio * num_entries))\n",
        "    train_idx, valid_idx = indices[:split], indices[split:]\n",
        "    \n",
        "    \n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "            dataset, sampler=train_sampler,\n",
        "            batch_size=batch_size)\n",
        "    \n",
        "    valid_loader = DataLoader(\n",
        "            dataset, sampler=valid_sampler,\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    return train_loader, valid_loader\n",
        "    \n",
        "\n",
        "model = Model (4 , 2 , 264 , 2)\n",
        "\n",
        "\n",
        "dataset = SeqData()\n",
        "\n",
        "train_loader , test_loader = split_rand(dataset, batch_size = 16)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for _ in range(300):\n",
        "    print(train(model, train_loader, optimizer, criterion))\n",
        "    \n",
        "    print(eval_net(model, test_loader, criterion))\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6965733051300049\n",
            "(0.7009388506412506, 0.40625)\n",
            "0.6927249789237976\n",
            "(0.7114456593990326, 0.40625)\n",
            "0.6918678402900695\n",
            "(0.7113458514213562, 0.40625)\n",
            "0.6882955074310303\n",
            "(0.7092073559761047, 0.40625)\n",
            "0.6900540828704834\n",
            "(0.7120343744754791, 0.40625)\n",
            "0.6872353792190552\n",
            "(0.7117859125137329, 0.40625)\n",
            "0.6881177186965942\n",
            "(0.7116632759571075, 0.40625)\n",
            "0.689265513420105\n",
            "(0.7069198489189148, 0.40625)\n",
            "0.6808666586875916\n",
            "(0.7001826465129852, 0.40625)\n",
            "0.6886512637138367\n",
            "(0.6845649480819702, 0.625)\n",
            "0.6861053705215454\n",
            "(0.6832996308803558, 0.625)\n",
            "0.6828104615211487\n",
            "(0.6893715262413025, 0.5625)\n",
            "0.6761790037155151\n",
            "(0.6878548562526703, 0.5625)\n",
            "0.6719621777534485\n",
            "(0.6889884769916534, 0.53125)\n",
            "0.6517516016960144\n",
            "(0.651681661605835, 0.65625)\n",
            "0.6851989507675171\n",
            "(0.6170934438705444, 0.65625)\n",
            "0.6369097352027893\n",
            "(0.6626282930374146, 0.625)\n",
            "0.6562268853187561\n",
            "(0.6374286413192749, 0.625)\n",
            "0.6038472414016723\n",
            "(0.6227944791316986, 0.65625)\n",
            "0.5887384653091431\n",
            "(0.5791071653366089, 0.6875)\n",
            "0.5691644012928009\n",
            "(0.530457466840744, 0.71875)\n",
            "0.6438547432422638\n",
            "(0.5989302694797516, 0.625)\n",
            "0.6449073195457459\n",
            "(0.6675756871700287, 0.625)\n",
            "0.6899944305419922\n",
            "(0.681696742773056, 0.4375)\n",
            "0.6860797524452209\n",
            "(0.686028927564621, 0.46875)\n",
            "0.6768752574920655\n",
            "(0.6884380280971527, 0.4375)\n",
            "0.6637092232704163\n",
            "(0.6875993013381958, 0.46875)\n",
            "0.6565909862518311\n",
            "(0.6787646114826202, 0.46875)\n",
            "0.6406627655029297\n",
            "(0.6736877262592316, 0.46875)\n",
            "0.6013638734817505\n",
            "(0.6234454214572906, 0.6875)\n",
            "0.5592173576354981\n",
            "(0.6783369481563568, 0.53125)\n",
            "0.5211897313594818\n",
            "(0.6610167920589447, 0.625)\n",
            "0.580236291885376\n",
            "(0.6199908554553986, 0.59375)\n",
            "0.570724356174469\n",
            "(0.6476875841617584, 0.5625)\n",
            "0.535501879453659\n",
            "(0.6303462386131287, 0.59375)\n",
            "0.5368293166160584\n",
            "(0.6338134706020355, 0.65625)\n",
            "0.5952633380889892\n",
            "(0.6634333431720734, 0.59375)\n",
            "0.5074514210224151\n",
            "(0.6412718892097473, 0.6875)\n",
            "0.5618293464183808\n",
            "(0.6237266361713409, 0.625)\n",
            "0.5112284004688263\n",
            "(0.6785137057304382, 0.53125)\n",
            "0.5643582105636596\n",
            "(0.6608150899410248, 0.5625)\n",
            "0.5114383399486542\n",
            "(0.6272791922092438, 0.65625)\n",
            "0.49586384892463686\n",
            "(0.6482592821121216, 0.65625)\n",
            "0.4753805696964264\n",
            "(0.6767086088657379, 0.625)\n",
            "0.4516465485095978\n",
            "(0.6409368515014648, 0.65625)\n",
            "0.4585240364074707\n",
            "(0.672604501247406, 0.6875)\n",
            "0.4240523993968964\n",
            "(0.7641421556472778, 0.6875)\n",
            "0.4541358292102814\n",
            "(0.6854420602321625, 0.71875)\n",
            "0.4497071266174316\n",
            "(0.7513989508152008, 0.625)\n",
            "0.4138141065835953\n",
            "(0.7546553313732147, 0.625)\n",
            "0.4196031093597412\n",
            "(0.6415067315101624, 0.59375)\n",
            "0.4873260915279388\n",
            "(0.5694885551929474, 0.71875)\n",
            "0.48288279175758364\n",
            "(0.5439165830612183, 0.6875)\n",
            "0.4313716471195221\n",
            "(0.542958527803421, 0.65625)\n",
            "0.4443784713745117\n",
            "(0.5349057167768478, 0.75)\n",
            "0.42890497446060183\n",
            "(0.5011169016361237, 0.75)\n",
            "0.410933381319046\n",
            "(0.4511175900697708, 0.78125)\n",
            "0.3752248704433441\n",
            "(0.38480043411254883, 0.8125)\n",
            "0.3554781287908554\n",
            "(0.3600113242864609, 0.84375)\n",
            "0.4773739278316498\n",
            "(0.6275802552700043, 0.6875)\n",
            "0.4141747713088989\n",
            "(0.5259276926517487, 0.75)\n",
            "0.40813218951225283\n",
            "(0.5227226614952087, 0.71875)\n",
            "0.336957773566246\n",
            "(0.5520828068256378, 0.6875)\n",
            "0.35793010592460633\n",
            "(0.647490918636322, 0.625)\n",
            "0.3364572674036026\n",
            "(0.4567277580499649, 0.8125)\n",
            "0.3254808306694031\n",
            "(0.5150338858366013, 0.78125)\n",
            "0.295960408449173\n",
            "(0.4057030528783798, 0.90625)\n",
            "0.35037767291069033\n",
            "(0.47148337960243225, 0.78125)\n",
            "0.25859640538692474\n",
            "(0.5535316467285156, 0.8125)\n",
            "0.28553159534931183\n",
            "(0.4053998589515686, 0.75)\n",
            "0.36630168855190276\n",
            "(0.45954981446266174, 0.8125)\n",
            "0.33396864533424375\n",
            "(0.7439829111099243, 0.75)\n",
            "0.31720419228076935\n",
            "(0.7140910625457764, 0.625)\n",
            "0.3094507873058319\n",
            "(0.7846683859825134, 0.59375)\n",
            "0.2993044346570969\n",
            "(0.6592615395784378, 0.71875)\n",
            "0.27793903946876525\n",
            "(0.9775838553905487, 0.59375)\n",
            "0.2921415001153946\n",
            "(0.6242174506187439, 0.71875)\n",
            "0.3941658779978752\n",
            "(0.5272417068481445, 0.6875)\n",
            "0.3226258963346481\n",
            "(0.5335147976875305, 0.6875)\n",
            "0.18868470638990403\n",
            "(0.22277547791600227, 0.90625)\n",
            "0.19529290795326232\n",
            "(0.4512384682893753, 0.875)\n",
            "0.22676396667957305\n",
            "(0.5490457564592361, 0.84375)\n",
            "0.19391910135746002\n",
            "(0.6095469743013382, 0.78125)\n",
            "0.12484426274895669\n",
            "(0.5436130464076996, 0.84375)\n",
            "0.08561186417937279\n",
            "(0.5074867308139801, 0.84375)\n",
            "0.05032015480101108\n",
            "(0.4073900729417801, 0.875)\n",
            "0.026565262861549855\n",
            "(0.37910282611846924, 0.90625)\n",
            "0.017866337113082408\n",
            "(0.4279572069644928, 0.90625)\n",
            "0.01034564096480608\n",
            "(0.42685389518737793, 0.90625)\n",
            "0.00805575717240572\n",
            "(0.3418709635734558, 0.9375)\n",
            "0.006839799741283059\n",
            "(0.3465588754042983, 0.96875)\n",
            "0.007176550617441535\n",
            "(0.35308387130498886, 0.9375)\n",
            "0.003636051423382014\n",
            "(0.3718310296535492, 0.90625)\n",
            "0.010745568713173271\n",
            "(0.3738491777330637, 0.9375)\n",
            "0.041144976299256085\n",
            "(0.3032286688685417, 0.96875)\n",
            "0.013369358773343266\n",
            "(0.3971642693504691, 0.9375)\n",
            "0.04450310741085559\n",
            "(0.36235448718070984, 0.9375)\n",
            "0.08670417971443385\n",
            "(1.2973108887672424, 0.78125)\n",
            "1.165952566312626\n",
            "(0.6431050300598145, 0.78125)\n",
            "0.9765926003456116\n",
            "(0.9789154231548309, 0.625)\n",
            "0.41131065785884857\n",
            "(0.43812116980552673, 0.78125)\n",
            "0.4382828831672668\n",
            "(0.4078254848718643, 0.8125)\n",
            "0.31496199369430544\n",
            "(0.37440214306116104, 0.8125)\n",
            "0.2575480043888092\n",
            "(0.3513057976961136, 0.84375)\n",
            "0.16281696259975434\n",
            "(0.34054916352033615, 0.84375)\n",
            "0.09899278804659843\n",
            "(0.34335026144981384, 0.84375)\n",
            "0.5562564074993134\n",
            "(0.5373131334781647, 0.84375)\n",
            "0.3105549141764641\n",
            "(0.6729350835084915, 0.71875)\n",
            "0.38494833111763\n",
            "(0.6562648415565491, 0.6875)\n",
            "0.2382838487625122\n",
            "(0.5479105859994888, 0.75)\n",
            "0.223575721681118\n",
            "(0.5102123320102692, 0.78125)\n",
            "0.17793900668621063\n",
            "(0.5199930667877197, 0.6875)\n",
            "0.1375570386648178\n",
            "(0.5465711057186127, 0.6875)\n",
            "0.10834546759724617\n",
            "(0.5981839895248413, 0.6875)\n",
            "0.07154944762587548\n",
            "(0.6144309639930725, 0.71875)\n",
            "0.050590109825134275\n",
            "(0.6277452707290649, 0.75)\n",
            "0.04197820797562599\n",
            "(0.6355841606855392, 0.75)\n",
            "0.03014766126871109\n",
            "(0.6319724023342133, 0.71875)\n",
            "0.022762225940823555\n",
            "(0.6281048059463501, 0.78125)\n",
            "0.019450272619724273\n",
            "(0.6407578587532043, 0.75)\n",
            "0.015428838320076466\n",
            "(0.6611740589141846, 0.75)\n",
            "0.012651996687054634\n",
            "(0.6772254705429077, 0.75)\n",
            "0.010864551831036805\n",
            "(0.6947853565216064, 0.75)\n",
            "0.009092287393286824\n",
            "(0.708673357963562, 0.75)\n",
            "0.00829201890155673\n",
            "(0.7250312864780426, 0.75)\n",
            "0.006938793882727623\n",
            "(0.7336383163928986, 0.75)\n",
            "0.006649697851389647\n",
            "(0.7424320578575134, 0.75)\n",
            "0.0055615843273699285\n",
            "(0.7486712038516998, 0.75)\n",
            "0.005069746263325214\n",
            "(0.7549930512905121, 0.75)\n",
            "0.004553820705041289\n",
            "(0.76058654114604, 0.75)\n",
            "0.004436821443960071\n",
            "(0.7666706144809723, 0.75)\n",
            "0.003864923259243369\n",
            "(0.7722169160842896, 0.75)\n",
            "0.00363739188760519\n",
            "(0.7763571366667747, 0.75)\n",
            "0.0033226174768060447\n",
            "(0.7799197733402252, 0.75)\n",
            "0.0029656770639121532\n",
            "(0.7866400480270386, 0.75)\n",
            "0.0028527050511911512\n",
            "(0.7920274138450623, 0.75)\n",
            "0.002693159366026521\n",
            "(0.7957013249397278, 0.75)\n",
            "0.002478603250347078\n",
            "(0.8009441792964935, 0.75)\n",
            "0.0023791240062564613\n",
            "(0.8033042252063751, 0.75)\n",
            "0.0022603604476898907\n",
            "(0.8076978623867035, 0.75)\n",
            "0.002103950153104961\n",
            "(0.8126152008771896, 0.75)\n",
            "0.00206183479167521\n",
            "(0.8154516816139221, 0.75)\n",
            "0.0019647128647193313\n",
            "(0.8181392550468445, 0.75)\n",
            "0.0018564256373792888\n",
            "(0.8197280168533325, 0.75)\n",
            "0.001758781005628407\n",
            "(0.8220300450921059, 0.78125)\n",
            "0.001704791677184403\n",
            "(0.8253094553947449, 0.78125)\n",
            "0.0016404428286477923\n",
            "(0.8283575773239136, 0.78125)\n",
            "0.0015488063101656735\n",
            "(0.8313949853181839, 0.78125)\n",
            "0.0014979766681790351\n",
            "(0.8343144357204437, 0.78125)\n",
            "0.0014382056426256895\n",
            "(0.8374402523040771, 0.78125)\n",
            "0.001388174865860492\n",
            "(0.840213418006897, 0.78125)\n",
            "0.0012662910274229943\n",
            "(0.8439260125160217, 0.78125)\n",
            "0.0012576224515214562\n",
            "(0.8472068309783936, 0.78125)\n",
            "0.0012092176126316189\n",
            "(0.8508976399898529, 0.78125)\n",
            "0.0011519631720148028\n",
            "(0.8539804518222809, 0.78125)\n",
            "0.0012092347838915884\n",
            "(0.8576518297195435, 0.78125)\n",
            "0.0011109068989753723\n",
            "(0.8593789041042328, 0.78125)\n",
            "0.001082361553562805\n",
            "(0.8623436391353607, 0.78125)\n",
            "0.0010840201983228325\n",
            "(0.8641424775123596, 0.78125)\n",
            "0.001017484674230218\n",
            "(0.8644395172595978, 0.78125)\n",
            "0.0009978857124224305\n",
            "(0.8663108348846436, 0.78125)\n",
            "0.0009103764372412115\n",
            "(0.8672880828380585, 0.78125)\n",
            "0.0009277003351598978\n",
            "(0.8697286546230316, 0.78125)\n",
            "0.0008963743108324707\n",
            "(0.8712591528892517, 0.78125)\n",
            "0.0008747648680582643\n",
            "(0.8732710480690002, 0.78125)\n",
            "0.0008440006175078451\n",
            "(0.8752235770225525, 0.75)\n",
            "0.000844340689945966\n",
            "(0.8767006546258926, 0.75)\n",
            "0.0007969247992150486\n",
            "(0.8798061311244965, 0.75)\n",
            "0.0007774890051223338\n",
            "(0.8829682618379593, 0.75)\n",
            "0.000747129149385728\n",
            "(0.8848716020584106, 0.75)\n",
            "0.0007481280714273453\n",
            "(0.8873319029808044, 0.75)\n",
            "0.0007331056171096861\n",
            "(0.8898471891880035, 0.75)\n",
            "0.0007075972273014486\n",
            "(0.8935207426548004, 0.75)\n",
            "0.0006883108522742987\n",
            "(0.8960548639297485, 0.75)\n",
            "0.000674779387190938\n",
            "(0.8987940549850464, 0.75)\n",
            "0.000650991772999987\n",
            "(0.9011130332946777, 0.75)\n",
            "0.0006574312574230134\n",
            "(0.9038775563240051, 0.75)\n",
            "0.0006231585051864385\n",
            "(0.905751645565033, 0.75)\n",
            "0.0005949952639639378\n",
            "(0.907774031162262, 0.75)\n",
            "0.0006020482687745243\n",
            "(0.9098499715328217, 0.75)\n",
            "0.0005733152909670025\n",
            "(0.9120652079582214, 0.75)\n",
            "0.0005780231324024499\n",
            "(0.9136963486671448, 0.75)\n",
            "0.0005377800844144076\n",
            "(0.9162535071372986, 0.75)\n",
            "0.0005323286692146212\n",
            "(0.9191123843193054, 0.75)\n",
            "0.000520377530483529\n",
            "(0.9225574359297752, 0.75)\n",
            "0.0005327729100827128\n",
            "(0.9257314503192902, 0.75)\n",
            "0.0004984223924111575\n",
            "(0.9272130876779556, 0.75)\n",
            "0.0005119424604345113\n",
            "(0.9298104345798492, 0.75)\n",
            "0.00046345504524651914\n",
            "(0.9326845407485962, 0.75)\n",
            "0.00047422500792890785\n",
            "(0.9353923201560974, 0.75)\n",
            "0.00046149095287546517\n",
            "(0.9383693933486938, 0.75)\n",
            "0.000486042961711064\n",
            "(0.9415769279003143, 0.75)\n",
            "0.0004552552127279341\n",
            "(0.9437875747680664, 0.75)\n",
            "0.0004506083787418902\n",
            "(0.9455548524856567, 0.75)\n",
            "0.00041807421075645834\n",
            "(0.9481165111064911, 0.75)\n",
            "0.0004278695909306407\n",
            "(0.9505628421902657, 0.75)\n",
            "0.0004109596076887101\n",
            "(0.9522003829479218, 0.75)\n",
            "0.0004329724004492164\n",
            "(0.955253392457962, 0.75)\n",
            "0.0004049358423799276\n",
            "(0.9583587050437927, 0.75)\n",
            "0.0003871196298860013\n",
            "(0.9607959538698196, 0.75)\n",
            "0.00037071193510200827\n",
            "(0.9626828730106354, 0.75)\n",
            "0.0003730656928382814\n",
            "(0.9651812613010406, 0.75)\n",
            "0.0003753808967303485\n",
            "(0.9685178697109222, 0.75)\n",
            "0.00036301064537838104\n",
            "(0.9700111746788025, 0.75)\n",
            "0.00036669545515906067\n",
            "(0.9717540144920349, 0.75)\n",
            "0.00035502888495102524\n",
            "(0.9751495122909546, 0.75)\n",
            "0.0003477181278867647\n",
            "(0.9776162505149841, 0.75)\n",
            "0.00033713944139890373\n",
            "(0.9794647693634033, 0.75)\n",
            "0.0003234149335185066\n",
            "(0.9822359681129456, 0.75)\n",
            "0.00033481173450127243\n",
            "(0.9841902256011963, 0.75)\n",
            "0.0003123227448668331\n",
            "(0.9857379794120789, 0.75)\n",
            "0.0003227934066671878\n",
            "(0.9871585667133331, 0.75)\n",
            "0.00031272673222701994\n",
            "(0.9894527792930603, 0.75)\n",
            "0.0003273484588135034\n",
            "(0.9912731945514679, 0.75)\n",
            "0.0003000947763212025\n",
            "(0.9929087162017822, 0.75)\n",
            "0.0003103945928160101\n",
            "(0.9945760667324066, 0.75)\n",
            "0.0002781874485663138\n",
            "(0.9964123070240021, 0.75)\n",
            "0.0002942792605608702\n",
            "(0.9975884854793549, 0.75)\n",
            "0.00029137722740415486\n",
            "(0.9993672668933868, 0.75)\n",
            "0.00028493832505773755\n",
            "(0.999614417552948, 0.75)\n",
            "0.00027098597784060987\n",
            "(1.0007992684841156, 0.75)\n",
            "0.00027172144036740064\n",
            "(1.0027123093605042, 0.75)\n",
            "0.00025871720863506197\n",
            "(1.0033251643180847, 0.75)\n",
            "0.00027331977034918966\n",
            "(1.0052850991487503, 0.75)\n",
            "0.0002623990963911638\n",
            "(1.0062590837478638, 0.75)\n",
            "0.00025862136099021884\n",
            "(1.0078828632831573, 0.75)\n",
            "0.00025400749582331627\n",
            "(1.0078043937683105, 0.75)\n",
            "0.0002456597809214145\n",
            "(1.0086099207401276, 0.75)\n",
            "0.00023984263534657658\n",
            "(1.009745478630066, 0.75)\n",
            "0.0002451367414323613\n",
            "(1.0112762153148651, 0.75)\n",
            "0.00023392639122903346\n",
            "(1.0124089121818542, 0.75)\n",
            "0.00023372310970444233\n",
            "(1.0146838128566742, 0.75)\n",
            "0.000233536257292144\n",
            "(1.016364336013794, 0.75)\n",
            "0.0002314448356628418\n",
            "(1.017636090517044, 0.75)\n",
            "0.00023922030522953718\n",
            "(1.0183333456516266, 0.75)\n",
            "0.00021188118553254752\n",
            "(1.0198806524276733, 0.78125)\n",
            "0.00021795221837237478\n",
            "(1.02102792263031, 0.78125)\n",
            "0.00021871032658964394\n",
            "(1.0222448110580444, 0.78125)\n",
            "0.000210475770290941\n",
            "(1.0241001844406128, 0.78125)\n",
            "0.00021505393960978835\n",
            "(1.0257092714309692, 0.78125)\n",
            "0.00020382660150062293\n",
            "(1.025956928730011, 0.78125)\n",
            "0.00020103788410779088\n",
            "(1.027537614107132, 0.78125)\n",
            "0.00019521439098753036\n",
            "(1.0291139483451843, 0.78125)\n",
            "0.00020040406088810413\n",
            "(1.0299886465072632, 0.78125)\n",
            "0.00019910456176148729\n",
            "(1.0315440893173218, 0.78125)\n",
            "0.00020083241834072395\n",
            "(1.0326368510723114, 0.78125)\n",
            "0.00019621670653577895\n",
            "(1.0323440432548523, 0.78125)\n",
            "0.00020198164565954357\n",
            "(1.033302366733551, 0.78125)\n",
            "0.0001773208423401229\n",
            "(1.0348745286464691, 0.78125)\n",
            "0.00019119214994134381\n",
            "(1.036984384059906, 0.78125)\n",
            "0.00017854932229965925\n",
            "(1.0381787419319153, 0.78125)\n",
            "0.00017695533606456592\n",
            "(1.0394265949726105, 0.78125)\n",
            "0.00018035910397884436\n",
            "(1.0409730970859528, 0.78125)\n",
            "0.0001721793640172109\n",
            "(1.0427187383174896, 0.78125)\n",
            "0.00018340755341341718\n",
            "(1.0442419648170471, 0.78125)\n",
            "0.00017272498225793243\n",
            "(1.0455169081687927, 0.78125)\n",
            "0.00016411366814281792\n",
            "(1.046970546245575, 0.78125)\n",
            "0.00016992605524137616\n",
            "(1.0478110909461975, 0.78125)\n",
            "0.0001727433322230354\n",
            "(1.0492055118083954, 0.78125)\n",
            "0.00016537197516299784\n",
            "(1.0510381758213043, 0.78125)\n",
            "0.00017411694425391034\n",
            "(1.0524466633796692, 0.78125)\n",
            "0.0001553491543745622\n",
            "(1.0527326464653015, 0.78125)\n",
            "0.00015484591567656026\n",
            "(1.0540140867233276, 0.78125)\n",
            "0.00016051136335590855\n",
            "(1.0552531480789185, 0.78125)\n",
            "0.0001517997749033384\n",
            "(1.0567049980163574, 0.78125)\n",
            "0.0001504822779679671\n",
            "(1.0584661960601807, 0.78125)\n",
            "0.00014950842887628824\n",
            "(1.0600517988204956, 0.78125)\n",
            "0.00014845543482806535\n",
            "(1.0615655481815338, 0.78125)\n",
            "0.00014154963282635434\n",
            "(1.0625645220279694, 0.78125)\n",
            "0.00014587966725230218\n",
            "(1.0637084245681763, 0.78125)\n",
            "0.00014760947378817947\n",
            "(1.0650621950626373, 0.78125)\n",
            "0.00014746058295713737\n",
            "(1.0665083825588226, 0.78125)\n",
            "0.00013702638680115342\n",
            "(1.0677284598350525, 0.78125)\n",
            "0.00014014251355547457\n",
            "(1.0693219900131226, 0.78125)\n",
            "0.00013792413228657096\n",
            "(1.0712340474128723, 0.78125)\n",
            "0.00013516502804122865\n",
            "(1.072864145040512, 0.78125)\n",
            "0.00013784907787339762\n",
            "(1.0744288563728333, 0.78125)\n",
            "0.00013505770621122793\n",
            "(1.0754419267177582, 0.78125)\n",
            "0.00013078019983367995\n",
            "(1.0766639709472656, 0.78125)\n",
            "0.00013187685835873707\n",
            "(1.0781849771738052, 0.78125)\n",
            "0.0001333696229266934\n",
            "(1.079419106245041, 0.78125)\n",
            "0.00012496142589952798\n",
            "(1.0813844799995422, 0.78125)\n",
            "0.0001275233051273972\n",
            "(1.0828124284744263, 0.78125)\n",
            "0.00012976447178516536\n",
            "(1.0834970772266388, 0.78125)\n",
            "0.00012463965686038136\n",
            "(1.084535390138626, 0.78125)\n",
            "0.00012025631731376052\n",
            "(1.0856255888938904, 0.78125)\n",
            "0.00012258628266863524\n",
            "(1.0866308212280273, 0.78125)\n",
            "0.00012374996003927663\n",
            "(1.0871807485818863, 0.78125)\n",
            "0.00012135962024331092\n",
            "(1.0888660550117493, 0.78125)\n",
            "0.00012261398369446398\n",
            "(1.0897535681724548, 0.78125)\n",
            "0.00011369441635906696\n",
            "(1.0911105573177338, 0.78125)\n",
            "0.00011991628562100232\n",
            "(1.092376470565796, 0.78125)\n",
            "0.00011590808571781963\n",
            "(1.0936086773872375, 0.78125)\n",
            "0.00011777099571190775\n",
            "(1.0953136086463928, 0.78125)\n",
            "0.00010825028075487353\n",
            "(1.0962736010551453, 0.78125)\n",
            "0.00011108406615676359\n",
            "(1.0973389744758606, 0.78125)\n",
            "0.00011166965996380895\n",
            "(1.0989616513252258, 0.78125)\n",
            "0.00010586879070615396\n",
            "(1.0995754301548004, 0.78125)\n",
            "0.00010395859790151008\n",
            "(1.100300908088684, 0.78125)\n",
            "0.00010868119279621169\n",
            "(1.101494014263153, 0.78125)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}